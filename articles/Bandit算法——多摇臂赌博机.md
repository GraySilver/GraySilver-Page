## Bandit算法——多摇臂赌博机

#### 选择问题

1. 假设一个用户对不同类别的兴趣内容感兴趣，那么我们推荐系统初次见到该用户怎么判断他对每类内容的感兴趣程度，这是推荐系统的冷启动问题。
2. 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？
3. ...

全是关于选择的问题，只要是一个选择的问题，就可以套进多臂赌博机来解释。当然，这也是一个EE问题，Exploit－Explore。

>exploit：把握已确定的收益，在确定的收益中选最大的。
>
>explore：不断探索新的领域，或者有更大的收益也说不定。

#### Bandit算法族谱

多臂问题有个概念叫做累计遗憾(regret)。

![img](http://x-algo.cn/wp-content/uploads/2016/12/cRegret.png)

其中，这里假定选择每个item的回报为伯努利回报。T是次数，$w_{B(i)}$是每一次item的期望收益，$w_{opt}$是每一次中所有items中的最佳选择的收益，$R_t$是累计T次的选择后的累计Regret，每次Regret的计算是最优选择收益和期望收益之差。

##### Thompson sampling

> 假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为p。
>
> 我们不断地试验，去估计出一个置信度较高的概率p的概率分布就能近似解决这个问题了。
>
> 怎么能估计概率p的概率分布呢？ 答案是假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose。
>
> 每个臂都维护一个beta分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。
>
> 每次选择臂的方式是：用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。

以上就是Thompson采样，用python实现很简单：

```python
randomChoiceList = []
for b in choiceList:
	choice = pymc.rbeta(1 + b['wins'], 1 + b['trials'] - b['wins'])
    randomChoiceList.append(choice)
maxChoice = np.argmax(randomChoiceList)
```

**相比于UCB算法，Thompson sampling：**

- UCB采用确定的选择策略，可能导致每次返回结果相同（不是推荐想要的），Thompson Sampling则是随机化策略。
- Thompson sampling实现相对更简单，UCB计算量更大（可能需要离线/异步计算）
- 在计算机广告、文章推荐领域，效果与UCB不相上下或更好competitive to or better
- 对于数据延迟反馈、批量数据反馈更加稳健robust

##### UCB

UCB算法全称是Upper Confidence Bound(置信区间上界)，不多说了，它的算法步骤如下:

> 先对每一个臂都试一遍；
>
> 之后，每次选择以下值最大的那个臂；
>
> ![img](https://pic4.zhimg.com/50/0713d95c7d24c1ef3ec00c03eb28503b_hd.jpg)
>
> 其中加号前面是这个臂到目前的收益均值，后面的叫做bonus，本质上是均值的标准差，t是目前的试验次数，Tjt是这个臂被试次数。
>
> 这个公式反映：均值越大，标准差越小，被选中的概率会越来越大，起到了exploit的作用；同时哪些被选次数较少的臂也会得到试验机会，起到了explore的作用。

与ε-Greedy算法、softmax算法相比，这种策略的好处在于：

- 考虑了回报均值的不确定性，让新的item更快得到尝试机会，将探索+开发融为一体
- 基础的UCB算法不需要任何参数，因此不需要考虑如何验证参数（ε如何确定）的问题

UCB1算法的缺点：

- UCB1算法需要首先尝试一遍所有item，因此当item数量很多时是一个问题
- 一开始各item选择次数都比较少，导致得到的回报波动较大（经常选中实际比较差的item）



##### Epsilon-Greedy

> 选一个(0,1)之间较小的数epsilon
>
> 每次以概率epsilon（产生一个[0,1]之间的随机数，比epsilon小）做一件事：所有臂中随机选一个。否则，选择截止当前，平均收益最大的那个臂。
>
> 是不是简单粗暴？epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只想花钱不想挣钱。

这样做的好处在于：

- 能够应对变化：如果item的回报发生变化，能及时改变策略，避免卡在局部极小值
- 可以控制对Exploration和Exploitation的偏好程度：ε大，模型具有更大的灵活性（能更快的探索潜在可能高回报item，适应变化，收敛速度更快），ε小，模型具有更好的稳定性（更多的机会用来开发利用当前最好回报的item），收敛速度变慢

虽然ε-Greedy算法在Exploration和Exploitation之间做出了一定平衡，但

- 设置最好的ε比较困难，大则适应变化较快，但长期累积回报低，小则适应变好的能力不够，但能获取更好的长期回报；
- 策略运行一段时间后，我们对item的好坏了解的确定性增强，但仍然花费固定的精力去exploration，浪费本应该更多进行exploitation机会；
- 策略运行一段时间后，我们已经对各item有了一定程度了解，但没用利用这些信息，仍然不做任何区分地随机exploration（会选择到明显较差的item）。

**增量更新item的回报期望：**

由于每次决策都是基于各item的历史回报均值，那么随着决策次数的增多，存储历史回报的空间消耗会越来越大，计算历史回报平均值的计算量也会越来越大，为了提高效率，对于历史回报平均值的计算可以做一下变形：

![img](http://x-algo.cn/wp-content/uploads/2016/12/increase-1.png)

##### Random

> 先试几次，每个臂都有了均值之后，一直选均值最大那个臂。这个算法是我们人类在实际中最常采用的，不可否认，它还是比随机乱猜要好。

![img](https://pic4.zhimg.com/50/2d8c17d65b09fb7ab0d77f31ff89aa37_hd.jpg)